 -*- coding: utf-8 -*-
"""
Author  : liyiyang (re-revised)
Purpose : SHAP analysis of human-activity drivers on water/carbon/food surplus
Outputs :
  - shap_values_<target>_<year>.csv            (city Ã— feature SHAP matrix)
  - shap_human_<target>_<year>.png             (per-year beeswarm)
  - shap_<target>_Average.png                  (stacked Average beeswarm)
  - <target>_SHAP_six_panel_grid.png           (2Ã—3 panel: 2000..2020 + Average)
  - shap_summary_<target>.csv                  (yearly mean SHAP table)
  - avg_contrib_<target>.png                   (mean-contrib barh)
  - clustermap_<target>.png                    (clustered heatmap)
"""

import re
import os
from pathlib import Path

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor,
    AdaBoostRegressor, StackingRegressor
)
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

import shap
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------- Matplotlib -----------------------------------
plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["font.sans-serif"] = ["Arial", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False
plt.rcParams["figure.dpi"] = 120

# ------------------------------ I/O è·¯å¾„ ------------------------------------
DATA_CSV = Path(r"G:/ç”Ÿæ€ç³»ç»ŸæŒ‡æ ‡/surplus.csv")      # â† ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
OUT_DIR   = Path("shap_outputs")
PANEL_DIR = OUT_DIR / "panels"
OUT_DIR.mkdir(parents=True, exist_ok=True)
PANEL_DIR.mkdir(parents=True, exist_ok=True)

# --------------------------- æ§åˆ¶é¡¹ï¼ˆé€Ÿåº¦/æ ‡é¢˜ï¼‰ -----------------------------
SEED = 42
BG_N = 100          # KernelExplainer èƒŒæ™¯æ ·æœ¬æ•°ï¼Œé™åˆ° 50 å¯åŠ é€Ÿ
NSAMPLES = "auto"   # æˆ–è®¾ä¸º 100~300 ä¹‹é—´æ•´æ•°
ADD_BEESWARM_TITLE = True     # å•å¼  beeswarm é¡¶éƒ¨æ˜¯å¦æ˜¾ç¤ºå¹´ä»½/â€œAverageâ€
SHOW_PANEL_TITLES  = True     # 2Ã—3 è”å›¾æ˜¯å¦æ˜¾ç¤º (a)(b)â€¦ å­å›¾æ ‡é¢˜

# ------------------------------ è¯»å…¥ä¸é¢„å¤„ç† --------------------------------
data = pd.read_csv(DATA_CSV)
data.columns = data.columns.str.lower().str.strip()
data.rename(columns={"2010atersurplus": "2010watersurplus"}, inplace=True)

def rear_year(col: str) -> str:
    m = re.match(r"^(\d{4})([a-z].*)$", col)
    return f"{m.group(2)}{m.group(1)}" if m else col

data.columns = [rear_year(c) for c in data.columns]

scaler = StandardScaler()
data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)

# ------------------------------ å˜é‡å®šä¹‰ ------------------------------------
human_features = ["ur", "nli", "gdp", "pigdp", "sigdp", "tigdp", "cla", "pd"]
targets = {
    "watersurplus": "Water Surplus",
    "carbonsurplus": "Carbon Surplus",
    "foodsurplus":   "Food Surplus"
}
years = [2000, 2005, 2010, 2015, 2020, 2023]

# ------------------------------ æ¨¡å‹é›†åˆ ------------------------------------
base_learners = [
    ("rf",   RandomForestRegressor(n_estimators=150, random_state=SEED, n_jobs=-1)),
    ("gb",   GradientBoostingRegressor(n_estimators=150, random_state=SEED)),
    ("xgb",  XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05,
                          subsample=0.8, colsample_bytree=0.8,
                          reg_lambda=1.0, random_state=SEED, verbosity=0, n_jobs=-1)),
    ("lgbm", LGBMRegressor(n_estimators=300, learning_rate=0.05, subsample=0.8,
                           colsample_bytree=0.8, random_state=SEED, verbose=-1, n_jobs=-1)),
    ("cat",  CatBoostRegressor(n_estimators=300, learning_rate=0.05, depth=6,
                               random_state=SEED, verbose=0)),
    ("ada",  AdaBoostRegressor(n_estimators=200, random_state=SEED)),
]
meta_learner = LinearRegression()

# ----------------------------- å·¥å…·å‡½æ•° --------------------------------------
def shap_one_year(X, y, feature_names, year, target_key, target_name, city_codes):
    """
    è®­ç»ƒå½“å¹´æ¨¡å‹ï¼Œè®¡ç®— Kernel SHAPï¼š
      - ä¿å­˜å½“å¹´ beeswarm å›¾
      - è¿”å›ï¼šæ ·æœ¬çº§ shap_values (ndarray)ã€X_short(DataFrame)ã€mean_shap(DataFrame)
    """
    # ç®€åŒ–åˆ—åï¼šUR2000 -> UR
    short_names = [f.replace(str(year), "") for f in feature_names]
    X_short = X.copy()
    X_short.columns = short_names

    model = StackingRegressor(estimators=base_learners, final_estimator=meta_learner, cv=5, n_jobs=-1)
    model.fit(X, y)

    bg = shap.sample(X, min(BG_N, len(X)), random_state=SEED)
    explainer = shap.KernelExplainer(lambda d: model.predict(pd.DataFrame(d, columns=feature_names)), bg)
    shap_values = explainer.shap_values(X, nsamples=NSAMPLES)

    # å•å¹´ beeswarm
    plt.figure()
    shap.summary_plot(shap_values, X_short, feature_names=short_names, show=False, plot_size=(7, 5))
    if ADD_BEESWARM_TITLE:
        plt.title(f"{year}", pad=8)
    f_png = OUT_DIR / f"shap_human_{target_key}_{year}.png"
    plt.savefig(f_png, dpi=220, bbox_inches="tight")
    plt.close()

    # åŸå¸‚Ã—ç‰¹å¾çŸ©é˜µ
    detail_df = pd.DataFrame(shap_values, columns=short_names, index=city_codes)
    detail_df.index.name = "citycode"
    detail_df.to_csv(OUT_DIR / f"shap_values_{target_key}_{year}.csv", encoding="utf-8-sig")

    # å¹´åº¦å‡å€¼è¡¨ï¼ˆåˆå¹¶æ±‡æ€»ç”¨ï¼‰
    mean_df = detail_df.mean(axis=0).reset_index().rename(columns={"index": "feature", 0: f"{year}"})

    return shap_values, X_short, mean_df, f_png

def merge_yearly(tbls):
    out = tbls[0]
    for t in tbls[1:]:
        out = pd.merge(out, t, on="feature", how="outer")
    return out.set_index("feature")

def make_six_panel(panel_pngs, panel_titles, out_path):
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.ravel()
    for ax, pth, ttl in zip(axes, panel_pngs, panel_titles):
        img = plt.imread(pth)
        ax.imshow(img)
        ax.axis("off")
        if SHOW_PANEL_TITLES:
            ax.set_title(ttl, fontsize=14, pad=6)
    plt.tight_layout()
    plt.savefig(out_path, dpi=220, bbox_inches="tight")
    plt.close()

# ------------------------------ ä¸»æµç¨‹ ---------------------------------------
# ä¸ºæ¯ä¸ªç›®æ ‡åˆ†åˆ«åšï¼š6 å¼ å•å¹´ + 1 å¼  Average + 2Ã—3 è”å›¾
for tgt_key, tgt_name in targets.items():
    print(f"\n==== Processing {tgt_name} ====")
    yearly_tables = []
    all_shap_arrays = []
    all_X_short = []
    peryear_imgs = []

    for year in years:
        human_cols = [f"{v}{year}" for v in human_features]
        tgt_col = f"{tgt_key}{year}"
        need_cols = human_cols + [tgt_col]

        if not set(need_cols).issubset(data_scaled.columns):
            print(f"âš ï¸  {tgt_key}-{year} ç¼ºå°‘åˆ—ï¼Œè·³è¿‡ï¼š{set(need_cols) - set(data_scaled.columns)}")
            continue

        df_y = data_scaled[need_cols].dropna()
        X = df_y[human_cols]
        y = df_y[tgt_col]

        city_codes = (data.loc[df_y.index, "citycode"]
                      if "citycode" in data.columns else df_y.index)

        shap_vals, X_short, mean_tbl, img_path = shap_one_year(
            X, y, human_cols, year, tgt_key, tgt_name, city_codes
        )
        yearly_tables.append(mean_tbl)
        all_shap_arrays.append(shap_vals)
        all_X_short.append(X_short)
        peryear_imgs.append(img_path)
        print(f"  âœ… {tgt_key}-{year} done")

    # Average é¢æ¿ï¼ˆæŠŠ 6 å¹´æ ·æœ¬çº§ SHAP ä¸ X çºµå‘å †å ï¼‰
    if len(all_shap_arrays) > 0:
        X_all = pd.concat(all_X_short, ignore_index=True)
        shap_all = np.vstack(all_shap_arrays)

        plt.figure()
        shap.summary_plot(shap_all, X_all, feature_names=X_all.columns.tolist(),
                          show=False, plot_size=(7, 5))
        if ADD_BEESWARM_TITLE:
            plt.title("Average", pad=8)
        avg_png = OUT_DIR / f"shap_{tgt_key}_Average.png"
        plt.savefig(avg_png, dpi=220, bbox_inches="tight")
        plt.close()
        peryear_imgs.append(avg_png)
    else:
        print(f"â— {tgt_key} æ— å¯ç”¨å¹´ä»½ï¼Œè·³è¿‡ Average/è”å›¾")
        continue

    # å¹´åº¦å‡å€¼è¡¨ä¸å¯è§†åŒ–
    df_tgt = merge_yearly(yearly_tables)
    df_tgt.to_csv(OUT_DIR / f"shap_summary_{tgt_key}.csv", encoding="utf-8-sig")

    plt.figure(figsize=(8, 6))
    df_tgt.mean(axis=1).sort_values().plot(kind="barh", color="skyblue")
    plt.axvline(0, color="gray", linestyle="--")
    plt.xlabel("Mean SHAP value")
    plt.title(f"Average contribution of human activities to {tgt_name}")
    plt.tight_layout()
    plt.savefig(OUT_DIR / f"avg_contrib_{tgt_key}.png", dpi=150, bbox_inches="tight")
    plt.close()

    sns.clustermap(df_tgt.dropna(), cmap="RdBu_r", center=0,
                   linewidths=0.5, col_cluster=False, figsize=(12, 8))
    plt.savefig(OUT_DIR / f"clustermap_{tgt_key}.png", dpi=160, bbox_inches="tight")
    plt.close()

    # 2Ã—3 è”å›¾ï¼ˆ2000..2020 + Averageï¼‰
    # è‹¥æŸäº›å¹´ä»½ç¼ºåˆ—è¢«è·³è¿‡ï¼Œæ­¤å¤„æŒ‰å·²æœ‰é¡ºåºæ‹¼æ¥
    titles = []
    for i, y in enumerate(years[:len(peryear_imgs)-1]):  # æœ€åä¸€å¼ æ˜¯ Average
        titles.append(f"({chr(97+i)}) {y}")
    titles.append(f"({chr(97+len(titles))}) Average")

    six_panel_path = PANEL_DIR / f"{tgt_key}_SHAP_six_panel_grid.png"
    make_six_panel(peryear_imgs, titles, six_panel_path)
    print(f"  ğŸ“Š saved 2Ã—3 panel â†’ {six_panel_path}")

print("\nğŸ‰ All SHAP analyses finished. Outputs saved in:", OUT_DIR.resolve())
